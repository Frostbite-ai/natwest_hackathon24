{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f8d96",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import joblib\n",
    "import datetime\n",
    "\n",
    "# Define paths\n",
    "TRAIN_DATA_PATH = \"../datasets/fraudTrain.csv\"\n",
    "TEST_DATA_PATH = \"../datasets/fraudTest.csv\"\n",
    "MODEL_DIR = \"../models/\"\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae12d74",
   "metadata": {},
   "source": [
    "# **1-Read and Truncate Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a7277d",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Read training data\n",
    "train_data = pd.read_csv(TRAIN_DATA_PATH)\n",
    "\n",
    "# Truncate training data to 10,000 samples for faster processing\n",
    "train_data = train_data.sample(n=10000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Training data shape after truncation: {train_data.shape}\")\n",
    "\n",
    "# Read test data\n",
    "test_data = pd.read_csv(TEST_DATA_PATH)\n",
    "\n",
    "# Truncate test data to 2,000 samples\n",
    "test_data = test_data.sample(n=2000, random_state=42).reset_index(drop=True)\n",
    "print(f\"Test data shape after truncation: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d13d6d6",
   "metadata": {},
   "source": [
    "# **2-Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae8fa4c",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Convert date columns to datetime if needed (optional since we're dropping them)\n",
    "# If retaining 'trans_date_trans_time' or 'dob', we can extract features like transaction hour, age, etc.\n",
    "\n",
    "# Drop PII and unnecessary columns\n",
    "columns_to_drop = ['Unnamed: 0', 'cc_num', 'first', 'last', 'street', 'city', 'state', 'zip', 'dob', 'trans_num', 'trans_date_trans_time']\n",
    "train_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "test_data.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "\n",
    "# Handle missing values\n",
    "train_data.dropna(inplace=True, ignore_index=True)\n",
    "test_data.dropna(inplace=True, ignore_index=True)\n",
    "\n",
    "print(f\"Training data shape after dropping columns and NaNs: {train_data.shape}\")\n",
    "print(f\"Test data shape after dropping columns and NaNs: {test_data.shape}\")\n",
    "\n",
    "# Encode categorical variables using LabelEncoder\n",
    "categorical_features = ['merchant', 'category', 'gender', 'job']\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    train_data[col] = le.fit_transform(train_data[col])\n",
    "    test_data[col] = le.transform(test_data[col])\n",
    "    label_encoders[col] = le  # Save the encoder for later use\n",
    "\n",
    "# Save label encoders\n",
    "label_encoders_dir = os.path.join(MODEL_DIR, 'label_encoders')\n",
    "if not os.path.exists(label_encoders_dir):\n",
    "    os.makedirs(label_encoders_dir)\n",
    "\n",
    "for col, le in label_encoders.items():\n",
    "    joblib.dump(le, os.path.join(label_encoders_dir, f\"{col}_encoder.pkl\"))\n",
    "\n",
    "print(\"Categorical features encoded.\")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "numerical_features = ['amt', 'time']  # 'time' is assumed to be present\n",
    "\n",
    "train_data[numerical_features] = scaler.fit_transform(train_data[numerical_features])\n",
    "test_data[numerical_features] = scaler.transform(test_data[numerical_features])\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, os.path.join(MODEL_DIR, 'scaler.pkl'))\n",
    "print(\"Numerical features scaled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e9a31a",
   "metadata": {},
   "source": [
    "# **3-EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d2060e",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Visualize class imbalance\n",
    "exit_counts = train_data[\"is_fraud\"].value_counts()\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(exit_counts, labels=[\"No Fraud\", \"Fraud\"], autopct=\"%0.1f%%\", colors=['skyblue', 'salmon'])\n",
    "plt.title(\"Fraudulent Transactions Percentage\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a42d528",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(train_data.corr(), annot=False, cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a3bef1",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Distribution of Amount\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(train_data['amt'], bins=50, kde=True)\n",
    "plt.title(\"Transaction Amount Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783de88",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Time distribution (if 'time' is present)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(train_data['time'], bins=50, kde=True)\n",
    "plt.title(\"Transaction Time Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d04bf3f",
   "metadata": {},
   "source": [
    "# **4-Train the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92b38f",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "X = train_data.drop(columns=[\"is_fraud\"])\n",
    "Y = train_data[\"is_fraud\"]\n",
    "\n",
    "# Define test features and target\n",
    "X_test = test_data.drop(columns=[\"is_fraud\"])\n",
    "Y_test = test_data[\"is_fraud\"]\n",
    "\n",
    "# Initialize and train the model\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X, Y)\n",
    "\n",
    "# Evaluate model on training data\n",
    "train_accuracy = model.score(X, Y)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# Save the trained model\n",
    "model_version = \"v1\"\n",
    "model_path = os.path.join(MODEL_DIR, f\"model_{model_version}.pkl\")\n",
    "joblib.dump(model, model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff9fc7",
   "metadata": {},
   "source": [
    "# **5-Test the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f3cbe",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "precision = precision_score(Y_test, y_pred)\n",
    "recall = recall_score(Y_test, y_pred)\n",
    "f1 = f1_score(Y_test, y_pred)\n",
    "roc_auc = roc_auc_score(Y_test, y_pred)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Precision: {precision:.4f}\")\n",
    "print(f\"Test Recall: {recall:.4f}\")\n",
    "print(f\"Test F1 Score: {f1:.4f}\")\n",
    "print(f\"Test ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Save test predictions\n",
    "test_predictions = test_data.copy()\n",
    "test_predictions['predicted_fraud'] = y_pred\n",
    "test_predictions.to_csv(\"../data/test_predictions.csv\", index=False)\n",
    "print(\"Test predictions saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08002d",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Fraud\", \"Fraud\"])\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02329d85",
   "metadata": {
    "title": "[code]"
   },
   "outputs": [],
   "source": [
    "# Save evaluation metrics to a file for MLOps tracking\n",
    "metrics = {\n",
    "    'model_version': model_version,\n",
    "    'accuracy': accuracy,\n",
    "    'precision': precision,\n",
    "    'recall': recall,\n",
    "    'f1_score': f1,\n",
    "    'roc_auc': roc_auc,\n",
    "    'timestamp': datetime.datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "metrics_df = pd.DataFrame([metrics])\n",
    "metrics_df.to_csv(os.path.join(MODEL_DIR, f\"metrics_{model_version}.csv\"), index=False)\n",
    "print(\"Evaluation metrics saved.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
